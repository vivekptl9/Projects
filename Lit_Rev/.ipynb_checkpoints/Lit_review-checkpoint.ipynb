{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3267652c-2fa8-4ddc-9304-554950dceccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "12.1\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/Projects/Lit_Rev/LiRenv/lib/python3.11/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)  # Displays the version of CUDA that PyTorch is using\n",
    "print(torch.cuda.is_available())  # Check if PyTorch can access the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175aed15-f16e-4f3e-b68d-f00fb54cf4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading paper: Intravalley spin-polarized superconductivity in rhombohedral tetralayer graphene\n",
      "Downloaded: C:/Users/YourUsername/Documents/Papers/2409.06701v1.pdf\n",
      "Downloading paper: MAMBO -- An empirical galaxy and AGN mock catalogue for the exploitation of future surveys\n",
      "Downloaded: C:/Users/YourUsername/Documents/Papers/2409.06700v1.pdf\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Function to fetch arXiv papers\n",
    "def fetch_arxiv_papers(query, year, max_results=100):\n",
    "    try:\n",
    "        # Create a client instance\n",
    "        client = arxiv.Client()\n",
    "        \n",
    "        # Perform the search\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "        \n",
    "        papers = []\n",
    "        for result in client.results(search):\n",
    "            # Extract the publication date\n",
    "            publication_date = result.published.date()\n",
    "            publication_year = publication_date.year\n",
    "            \n",
    "            # Check if the publication year matches the specified year\n",
    "            if publication_year == year:\n",
    "                papers.append({\n",
    "                    \"title\": result.title,\n",
    "                    \"summary\": result.summary,\n",
    "                    \"url\": result.entry_id,\n",
    "                    \"publication_year\": publication_year\n",
    "                })\n",
    "        \n",
    "        if not papers:\n",
    "            print(f\"No papers found for query: {query} in year: {year}\")\n",
    "        return papers\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching papers: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to download and save the PDF of a paper\n",
    "def download_paper(paper_url, save_dir=\"papers\"):\n",
    "    try:\n",
    "        # Convert abstract URL to PDF URL\n",
    "        pdf_url = paper_url.replace('abs', 'pdf') + \".pdf\"\n",
    "        \n",
    "        # Get the paper ID to use as the filename\n",
    "        paper_id = paper_url.split('/')[-1]\n",
    "        pdf_filename = os.path.join(save_dir, f\"{paper_id}.pdf\")\n",
    "        \n",
    "        # Create the save directory if it doesn't exist\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Download the PDF\n",
    "        response = requests.get(pdf_url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        \n",
    "        # Save the PDF file locally\n",
    "        with open(pdf_filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        \n",
    "        print(f\"Downloaded: {pdf_filename}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {paper_url}: {e}\")\n",
    "\n",
    "# Fetch papers for a specific query and year\n",
    "year = 2024\n",
    "papers = fetch_arxiv_papers(\"particle physics\", year, max_results=2)\n",
    "\n",
    "# Download each paper as a PDF and save it on your PC\n",
    "for paper in papers:\n",
    "    print(f\"Downloading paper: {paper['title']}\")\n",
    "    download_paper(paper['url'], save_dir=\"C:/Users/YourUsername/Documents/Papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76ffeee-779f-40a6-a045-6ce6ae311f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# # Initialize Hugging Face's summarization pipeline using the installed framework\n",
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\",device =-1)\n",
    "\n",
    "# # Summarize the fetched papers\n",
    "# def summarize_paper(paper_summary):\n",
    "#     summary = summarizer(paper_summary, max_length=150, min_length=40, do_sample=False)\n",
    "#     return summary[0]['summary_text']\n",
    "\n",
    "# # Example usage\n",
    "# for paper in papers:\n",
    "#     summary = summarize_paper(paper['summary'])\n",
    "#     print(f\"Paper: {paper['title']}\\nSummary: {summary}\\nLink: {paper['url']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9914b9-f94a-413d-8792-27821f4b7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import torchaudio\n",
    "\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "# print(f\"torchvision version: {torchvision.__version__}\")\n",
    "# print(f\"torchaudio version: {torchaudio.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae3c10d-38ab-4238-871c-1b653079d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# # Initialize Hugging Face's summarization pipeline using CPU (device=-1 for CPU)\n",
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
    "\n",
    "# # Summarize the fetched papers\n",
    "# def summarize_paper(paper_summary):\n",
    "#     return summarizer(paper_summary, max_length=130, min_length=30, do_sample=False)\n",
    "\n",
    "# # Example paper summary\n",
    "# paper_summary = \"Particle physics is a branch of physics that studies the nature of particles that constitute matter and radiation...\"\n",
    "# summary = summarize_paper(paper_summary)\n",
    "# print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "992855db-9732-4581-96fe-5eda804eb136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD path is given by.pdf\n",
      "PDF not found: .pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/Projects/Lit_Rev/LiRenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import pipeline\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from fpdf import FPDF\n",
    "import pickle\n",
    "\n",
    "# Initialize Hugging Face's summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    try:\n",
    "        pdf = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the extracted text by removing irrelevant sections and formatting issues.\"\"\"\n",
    "    # Remove sections like acknowledgments and references\n",
    "    exclude_patterns = [\"Acknowledgments\", \"References\", \"Bibliography\"]\n",
    "    \n",
    "    for pattern in exclude_patterns:\n",
    "        pattern_match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if pattern_match:\n",
    "            text = text[:pattern_match.start()]\n",
    "            break\n",
    "\n",
    "    # Remove excessive spaces and broken words\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s*([.,;!?()])\\s*', r'\\1 ', text)  # Clean spaces around punctuation\n",
    "    text = re.sub(r'\\b(\\w+)-(\\w+)\\b', r'\\1\\2', text)  # Merge hyphenated words\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def summarize_paper(full_text):\n",
    "    \"\"\"Summarize the provided text using the summarization pipeline.\"\"\"\n",
    "    max_chunk_size = 1000\n",
    "    chunks = [full_text[i:i + max_chunk_size] for i in range(0, len(full_text), max_chunk_size)]\n",
    "    \n",
    "    full_summary = \"\"\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)\n",
    "            full_summary += summary[0]['summary_text'] + \" \"\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing text chunk: {e}\")\n",
    "    \n",
    "    return full_summary.strip()\n",
    "\n",
    "def download_pdf(pdf_url, download_path):\n",
    "    \"\"\"Download a PDF file from a URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        with open(download_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading PDF: {e}\")\n",
    "\n",
    "# Example usage\n",
    "# pdf_url = \"https://arxiv.org/pdf/2403.12178\"  # Replace with actual PDF URL\n",
    "# pdf_path = \"sample.pdf\"\n",
    "# download_pdf(pdf_url, pdf_path)\n",
    "\n",
    "def save_summary_as_pdf(summary, filename):\n",
    "    try:\n",
    "        # save_dir = \"C:/Users/YourUsername/Documents/Papers\"\n",
    "        # os.makedirs(save_dir, exist_ok=True)\n",
    "        completeName = os.path.join(filename) \n",
    "        print(f\"Saved summary as PDF: {filename}\")\n",
    "        file1 = open(completeName, \"w\")\n",
    "        file1.write(summary)\n",
    "        file1.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving summary as PDF: {e}\")\n",
    "\n",
    "# def save_summary_as_txt(summary, filename):\n",
    "#     with open('summary.pkl', 'wb') as file:\n",
    "#         pickle.dump(summary,file)\n",
    "#     with open('summary.pkl','rb') as file:\n",
    "#         summary_data = pickle.load(file)\n",
    "#     summary_str = str(summary_data)\n",
    "#     summary_op_path = filename\n",
    "#     with open(summary_op_path, 'w') as file:\n",
    "#         file.write(summary_str)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# filename = \"summary\"\n",
    "# save_summary_as_pdf(summary, filename)\n",
    "# dir_path = \"C:/Users/YourUsername/Documents/Papers\"\n",
    "# files = os.listdir(dir_path)\n",
    "# pdf_files = [file for file in files if file.lower().endswith('.pdf')]\n",
    "# for pdf_file in pdf_files:\n",
    "#     pdf_path = os.path.join(dir_path, pdf_file)\n",
    "#     if os.path.exists(pdf_path):\n",
    "#     # Extract and clean the text from the downloaded PDF\n",
    "#         pdf_text = extract_text_from_pdf(pdf_path)\n",
    "#         cleaned_text = clean_text(pdf_text)\n",
    "        \n",
    "#         # Summarize the cleaned text\n",
    "#         summary = summarize_paper(cleaned_text)\n",
    "#         #print(f\"Full Paper Summary: {summary}\")\n",
    "#         summary_pdf_path = os.path.join(f\"{paper['url'].split('/')[-1]}_summary.pdf\")\n",
    "#         save_summary_as_txt(summary, summary_pdf_path)\n",
    "#     else:\n",
    "#         print(f\"PDF not found: {pdf_path}\")\n",
    "\n",
    "\n",
    "for paper in papers:\n",
    "    dir_path = \"C:/Users/YourUsername/Documents/Papers\"\n",
    "    files = os.listdir(dir_path)\n",
    "    pdf_files = [file for file in files if file.lower().endswith('.pdf')]\n",
    "    pdf_path = os.path.join(f\"{paper['url'].split('/')[-1]}.pdf\")\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(dir_path, pdf_file)\n",
    "        print(f\"PAD path is given by{pdf_path}\")\n",
    "    if os.path.exists(pdf_path):\n",
    "    # Extract and clean the text from the downloaded PDF\n",
    "        pdf_text = extract_text_from_pdf(pdf_path)\n",
    "        cleaned_text = clean_text(pdf_text)\n",
    "        \n",
    "        # Summarize the cleaned text\n",
    "        summary = summarize_paper(cleaned_text)\n",
    "        #print(f\"Full Paper Summary: {summary}\")\n",
    "        summary_pdf_path = os.path.join(save_dir, f\"{paper['url'].split('/')[-1]}_summary.pdf\")\n",
    "        save_summary_as_pdf(summary, summary_pdf_path)\n",
    "    else:\n",
    "        print(f\"PDF not found: {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5f57648-aa1b-4d2a-a7c2-25d204db256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary as PDF: summary.pdf\n",
      "PDF not found: C:/Users/YourUsername/Documents/Papers/.pdf\n",
      "Saved TL;DR table as PDF: C:/Users/YourUsername/Documents/Papers/papers_tldr_summary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/Projects/Lit_Rev/LiRenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_32032/3142441296.py:72: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", size=12)\n",
      "/tmp/ipykernel_32032/3142441296.py:121: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", size=12)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from fpdf import FPDF\n",
    "from transformers import pipeline\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Initialize Hugging Face's summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    try:\n",
    "        pdf = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the extracted text by removing irrelevant sections and formatting issues.\"\"\"\n",
    "    # Remove sections like acknowledgments and references\n",
    "    exclude_patterns = [\"Acknowledgments\", \"References\", \"Bibliography\"]\n",
    "    \n",
    "    for pattern in exclude_patterns:\n",
    "        pattern_match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if pattern_match:\n",
    "            text = text[:pattern_match.start()]\n",
    "            break\n",
    "\n",
    "    # Remove excessive spaces and broken words\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s*([.,;!?()])\\s*', r'\\1 ', text)  # Clean spaces around punctuation\n",
    "    text = re.sub(r'\\b(\\w+)-(\\w+)\\b', r'\\1\\2', text)  # Merge hyphenated words\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def summarize_paper(full_text):\n",
    "    \"\"\"Summarize the provided text using the summarization pipeline.\"\"\"\n",
    "    max_chunk_size = 1000\n",
    "    chunks = [full_text[i:i + max_chunk_size] for i in range(0, len(full_text), max_chunk_size)]\n",
    "    \n",
    "    full_summary = \"\"\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)\n",
    "            full_summary += summary[0]['summary_text'] + \" \"\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing text chunk: {e}\")\n",
    "    \n",
    "    return full_summary.strip()\n",
    "\n",
    "def download_pdf(pdf_url, download_path):\n",
    "    \"\"\"Download a PDF file from a URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        with open(download_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading PDF: {e}\")\n",
    "\n",
    "def save_summary_as_pdf(summary, filename):\n",
    "    try:\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_auto_page_break(auto=True, margin=15)\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "        \n",
    "        # Use multi_cell to handle large amounts of text and preserve formatting\n",
    "        pdf.multi_cell(0, 10, summary)\n",
    "        \n",
    "        pdf.output(filename)\n",
    "        print(f\"Saved summary as PDF: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving summary as PDF: {e}\")\n",
    "\n",
    "# Example usage\n",
    "summary = \"Your summarized text here.\"\n",
    "filename = \"summary.pdf\"\n",
    "save_summary_as_pdf(summary, filename)\n",
    "def create_tldr(summary):\n",
    "    \"\"\"Create a TL;DR from the summary.\"\"\"\n",
    "    # Example TL;DR: first sentence of the summary (for demonstration purposes)\n",
    "    return summary.split('.')[0] + \".\"\n",
    "\n",
    "def save_tldr_table(papers, save_dir):\n",
    "    \"\"\"Save TL;DR and other paper details in a tabular form.\"\"\"\n",
    "    try:\n",
    "        data = []\n",
    "        for paper in papers:\n",
    "            pdf_path = os.path.join(save_dir, f\"{paper['url'].split('/')[-1]}.pdf\")\n",
    "            if os.path.exists(pdf_path):\n",
    "                pdf_text = extract_text_from_pdf(pdf_path)\n",
    "                cleaned_text = clean_text(pdf_text)\n",
    "                summary = summarize_paper(cleaned_text)\n",
    "                tldr = create_tldr(summary)\n",
    "                data.append({\n",
    "                    \"Title\": paper[\"title\"],\n",
    "                    \"URL\": paper[\"url\"],\n",
    "                    \"Summary\": summary,\n",
    "                    \"TL;DR\": tldr\n",
    "                })\n",
    "            else:\n",
    "                print(f\"PDF not found: {pdf_path}\")\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        table_pdf_path = os.path.join(save_dir, \"papers_tldr_summary.pdf\")\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(os.path.join(save_dir, \"papers_tldr_summary.csv\"), index=False)\n",
    "\n",
    "        # Save the DataFrame to a PDF file\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_auto_page_break(auto=True, margin=15)\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            pdf.cell(0, 10, f\"Title: {row['Title']}\", ln=True)\n",
    "            pdf.multi_cell(0, 10, f\"URL: {row['URL']}\")\n",
    "            pdf.multi_cell(0, 10, f\"Summary: {row['Summary']}\")\n",
    "            pdf.multi_cell(0, 10, f\"TL;DR: {row['TL;DR']}\")\n",
    "            pdf.ln(10)  # Add a line break\n",
    "\n",
    "        pdf.output(table_pdf_path)\n",
    "        print(f\"Saved TL;DR table as PDF: {table_pdf_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving TL;DR table: {e}\")\n",
    "\n",
    "# Example usage\n",
    "papers = [\n",
    "    {\n",
    "        \"title\": \"Sample Paper\",\n",
    "        \"url\": \"https://arxiv.org/abs/\"\n",
    "    }\n",
    "]\n",
    "\n",
    "save_dir = \"C:/Users/YourUsername/Documents/Papers\"\n",
    "save_tldr_table(papers, save_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
